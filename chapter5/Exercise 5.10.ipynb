{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "from copy import copy,deepcopy\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.10 小车race track (Off-policy Mc control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(metaclass=ABCMeta):\n",
    "    def __init__(self,rng_seed=random.randint(0,1000)):\n",
    "        self.rng = np.random.RandomState(rng_seed)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_actions(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self,action):\n",
    "        pass\n",
    "        # return done,observation,reward,extra\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_observation(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def observation_to_state(self,observation):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_history(self,):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_state_shape(self):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "class RaceTrackEnv(Env):\n",
    "    def __init__(self,plane,start_line,end_line,speed_limit=5):\n",
    "        super().__init__()\n",
    "        self.plane = plane\n",
    "        self.start_line = start_line\n",
    "        self.end_line = end_line\n",
    "        self.actions = [(i,j) for i in [-1,0,1] for j in [-1,0,1]]\n",
    "        self.reset()\n",
    "        self.speed_limit = speed_limit\n",
    "        self.end_dict = {}\n",
    "        for one_point in end_line:\n",
    "            self.end_dict[tuple(one_point)] = 1\n",
    "        self.w,self.h = plane.shape\n",
    "        \n",
    "        self.action2ind = {}\n",
    "        self.ind2action = {}\n",
    "        for i,one_action in enumerate(self.actions):\n",
    "            self.action2ind[one_action] = i\n",
    "            self.ind2action[i] = one_action\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def is_done(self,old_x,old_y):\n",
    "        for x in range(min(old_x,old_x + self.speed[0]),max(old_x,old_x + self.speed[0] + 1)):\n",
    "            if (x,old_y) in self.end_dict:\n",
    "                return True\n",
    "        \n",
    "        for y in range(min(old_y,old_y + self.speed[1]),max(old_y,old_y + self.speed[1] + 1)):\n",
    "            if (x,y) in self.end_dict:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def step(self,action):\n",
    "        observation_last_step = self.get_observation()\n",
    "        current_state = self.observation_to_state(observation_last_step)\n",
    "        alpha_x,alpha_y = action\n",
    "        self.history.append((copy(current_state),copy(action)))\n",
    "        \n",
    "        old_speed_y = self.speed[1]\n",
    "        old_speed_x = self.speed[0]\n",
    "        \n",
    "        self.speed[0] += alpha_x\n",
    "        if self.speed[0] < -self.speed_limit:self.speed[0] = -self.speed_limit\n",
    "        elif self.speed[0] > self.speed_limit:self.speed[0] = self.speed_limit\n",
    "            \n",
    "        self.speed[1] += alpha_y\n",
    "        if self.speed[1] < -self.speed_limit:self.speed[1] = -self.speed_limit\n",
    "        elif self.speed[1] > self.speed_limit:self.speed[1] = self.speed_limit\n",
    "            \n",
    "        if self.speed[0] == 0 and self.speed[1] == 0:\n",
    "            self.speed[1] = old_speed_y\n",
    "            self.speed[0] = -1\n",
    "        \n",
    "        old_x,old_y = self.position\n",
    "        new_x = old_x + self.speed[0]\n",
    "        new_y = old_y + self.speed[1]\n",
    "        \n",
    "        assert(not(self.speed[0] == 0 and self.speed[1] == 0))\n",
    "        \n",
    "        self.position = [new_x,new_y]\n",
    "        #print(old_x,old_y)\n",
    "        #print(\"speed\",self.speed)\n",
    "            \n",
    "        is_done = self.is_done(old_x,old_y)\n",
    "        if new_x < 0 or new_x >= self.plane.shape[0] or new_y < 0 or new_y >= self.plane.shape[1]\\\n",
    "            or self.plane[new_x,new_y] < 0:\n",
    "            #print(\"returning\")\n",
    "            self.return_to_start()\n",
    "        if is_done:\n",
    "            return True,self.get_observation(),1,None\n",
    "        else:\n",
    "            return False,self.get_observation(),0,None\n",
    "        \n",
    "        \n",
    "        # return done,observation,reward,extra\n",
    "        \n",
    "    def get_observation(self):\n",
    "        return {\n",
    "            \"plane\": self.plane,\n",
    "            \"position\": self.position,\n",
    "            \"speed\": self.speed,\n",
    "        }\n",
    "    \n",
    "    def observation_to_state(self,observation):\n",
    "        position = observation[\"position\"]\n",
    "        speed = observation[\"speed\"]\n",
    "        speedx,speedy = speed\n",
    "        assert(speedx >= -self.speed_limit and speedx <= self.speed_limit)\n",
    "        assert(speedy >= -self.speed_limit and speedy <= self.speed_limit)\n",
    "        return list(position) + [speedx + self.speed_limit,speedy + self.speed_limit]\n",
    "    \n",
    "    def get_history(self):\n",
    "        return history\n",
    "    \n",
    "    def get_state_shape(self):\n",
    "        return list(self.plane.shape) + [self.speed_limit * 2 + 1,self.speed_limit * 2 + 1]\n",
    "    \n",
    "    def return_to_start(self):\n",
    "        self.position = self.start_line[self.rng.randint(0,len(self.start_line))]\n",
    "        self.position = copy(self.position)\n",
    "        self.speed = [0,0]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.return_to_start()\n",
    "        self.history = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceTrackGenerator(object):\n",
    "    def __init__(self,width=32,height=32):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def generate(self):\n",
    "        self.plane = -np.ones([self.width,self.height])\n",
    "        self.plane[:,:9] = 0\n",
    "        self.plane[:6,:20] = 0\n",
    "        start_points = []\n",
    "        end_points = []\n",
    "        for i in range(9):\n",
    "            start_points.append([self.width - 1,i])\n",
    "            \n",
    "        for i in range(6):\n",
    "            end_points.append([i,19])\n",
    "            \n",
    "        self.start_points = start_points\n",
    "        self.end_points = end_points\n",
    "        \n",
    "        for one_point in start_points:\n",
    "            self.plane[one_point[0],one_point[1]] = 1\n",
    "            \n",
    "        for one_point in end_points:\n",
    "            self.plane[one_point[0],one_point[1]] = 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceTrackGenerator = RaceTrackGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceTrackGenerator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d33ccd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(raceTrackGenerator.plane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceTrackEnv = RaceTrackEnv(\n",
    "    raceTrackGenerator.plane,\n",
    "    raceTrackGenerator.start_points,\n",
    "    raceTrackGenerator.end_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceTrackEnv.reset()\n",
    "while True:\n",
    "    actions = random.choice(raceTrackEnv.get_actions())\n",
    "    done,observation,reward,extra = raceTrackEnv.step(actions)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3350"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raceTrackEnv.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([17, 1, 2, 6], (-1, 0))\n",
      "([13, 2, 1, 6], (1, -1))\n",
      "([10, 2, 2, 5], (0, -1))\n",
      "([7, 1, 2, 4], (-1, -1))\n",
      "([31, 6, 5, 5], (-1, 0))\n",
      "([30, 6, 4, 5], (0, 1))\n",
      "([29, 7, 4, 6], (1, -1))\n",
      "([28, 8, 4, 6], (-1, -1))\n",
      "([26, 8, 3, 5], (1, 1))\n",
      "([31, 6, 5, 5], (0, 0))\n",
      "([30, 6, 4, 5], (0, 0))\n",
      "([29, 6, 4, 5], (1, 0))\n",
      "([28, 6, 4, 5], (-1, 0))\n",
      "([26, 6, 3, 5], (0, 0))\n",
      "([24, 6, 3, 5], (0, 0))\n",
      "([22, 6, 3, 5], (0, 0))\n",
      "([20, 6, 3, 5], (0, -1))\n",
      "([18, 5, 3, 4], (-1, -1))\n",
      "([15, 3, 2, 3], (1, 1))\n",
      "([13, 2, 3, 4], (0, 1))\n",
      "([11, 2, 3, 5], (1, -1))\n",
      "([10, 1, 4, 4], (-1, 1))\n",
      "([8, 1, 3, 5], (0, 0))\n",
      "([6, 1, 3, 5], (1, 1))\n",
      "([5, 2, 4, 6], (0, 1))\n",
      "([4, 4, 4, 7], (0, 0))\n",
      "([3, 6, 4, 7], (0, 1))\n",
      "([2, 9, 4, 8], (1, 1))\n",
      "([2, 13, 5, 9], (1, 0))\n",
      "([3, 17, 6, 9], (-1, 0))\n"
     ]
    }
   ],
   "source": [
    "for one_step in raceTrackEnv.history[-30:]:\n",
    "    print(one_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### off policy 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(metaclass=ABCMeta):\n",
    "    def __init__(self,env:Env):\n",
    "        self.env = env\n",
    "    \n",
    "    @staticmethod\n",
    "    def action_probs(self):\n",
    "        pass\n",
    "    \n",
    "class EqualProbStrategy(Strategy):\n",
    "    def __init__(self,env:Env):\n",
    "        self.env = env\n",
    "        self.actions = self.env.get_actions()\n",
    "    \n",
    "    def action_probs(self):\n",
    "        retval = {}\n",
    "        for one_action in self.actions:\n",
    "            retval[one_action] = 1.0 / len(self.actions)\n",
    "        return retval\n",
    "    \n",
    "class TargetStrategy(Strategy):\n",
    "    def __init__(self,env:Env):\n",
    "        self.env = env\n",
    "        self.actions = self.env.get_actions()\n",
    "        self.best_strategy = np.zeros(self.env.get_state_shape())\n",
    "    \n",
    "    def action_probs(self):\n",
    "        state = self.env.observation_to_state(self.env.get_observation())\n",
    "        \n",
    "        action_to_select = self.best_strategy[tuple(state)]\n",
    "        \n",
    "        retval = {}\n",
    "        action_sum = 0\n",
    "        for action_ind,one_action in enumerate(self.actions):\n",
    "            if action_ind == action_to_select:\n",
    "                retval[one_action] = 1\n",
    "                action_sum += 1\n",
    "            else:\n",
    "                retval[one_action] = 0\n",
    "                action_sum += 0\n",
    "        assert(action_sum == 1)\n",
    "        return retval\n",
    "    \n",
    "    def set_best_strategy(self,state,action):\n",
    "        self.best_strategy[tuple(state)] = self.env.action2ind[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMcControl(metaclass=ABCMeta):\n",
    "    def __init__(self,env:Env,target_strategy:Strategy,behavior_strategy:Strategy):\n",
    "        self.env = env\n",
    "        self.target_strategy = target_strategy\n",
    "        self.behavior_strategy = behavior_strategy\n",
    "    \n",
    "    @abstractmethod\n",
    "    def set_env_random_initial_state(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def estimate_value(self,steps):\n",
    "        pass\n",
    "\n",
    "class RaceCarOptimizer(OffPolicyMcControl):\n",
    "    def __init__(self,env:RaceTrackEnv,target_strategy:Strategy,behavior_strategy:Strategy,\n",
    "                 discount=0.9):\n",
    "        super().__init__(env,target_strategy,behavior_strategy)\n",
    "        self.QAshape = self.env.get_state_shape() + [len(self.env.get_actions())]\n",
    "        self.discount = discount\n",
    "        self.actions = self.env.actions\n",
    "        \n",
    "        self.Q = np.zeros(self.QAshape,dtype=np.float)\n",
    "        self.C = np.zeros(self.QAshape,dtype=np.float)\n",
    "        \n",
    "    def set_env_random_initial_state(self):\n",
    "        self.env.reset()\n",
    "        \n",
    "    def backprog(self,infos):\n",
    "        (states,rewards,actions,prob_pis,prob_bes) = infos\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        \n",
    "        for one_state,one_reward,one_action,one_prob_pi,one_prob_be in zip(states[::-1],rewards[::-1],actions[::-1],prob_pis[::-1],prob_bes[::-1]):\n",
    "            G = G * self.discount + one_reward\n",
    "            \n",
    "            one_action_ind = self.env.action2ind[one_action]\n",
    "            state_action = tuple(one_state + [one_action_ind])\n",
    "            self.C[state_action] += W\n",
    "            self.Q[state_action] += (W / self.C[state_action]) * (G - self.Q[state_action])\n",
    "            \n",
    "            # TODO 此处仅考虑了reward为正数的情况\n",
    "            \n",
    "            best_action_ind = np.argmax(self.Q[tuple(one_state)])\n",
    "            self.target_strategy.set_best_strategy(one_state,self.actions[best_action_ind])\n",
    "            \n",
    "            if one_action_ind == best_action_ind:\n",
    "                W = W / one_prob_be\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    def monte_carlo_run(self):\n",
    "        self.set_env_random_initial_state()\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        prob_pis = []\n",
    "        prob_bes = []\n",
    "        while True:\n",
    "            state = self.env.observation_to_state(self.env.get_observation())\n",
    "            states.append(state)\n",
    "\n",
    "            nextBehaviorProbs = self.behavior_strategy.action_probs()\n",
    "            nextTargetProbs = self.target_strategy.action_probs()\n",
    "            \n",
    "            nextactions = list(nextBehaviorProbs.keys())\n",
    "            nextstep = np.random.choice(list(range(len(nextactions))),p=list(nextBehaviorProbs.values()))\n",
    "            nextstep = nextactions[nextstep]\n",
    "            \n",
    "            prob_bes.append(nextBehaviorProbs[nextstep])\n",
    "            prob_pis.append(nextTargetProbs[nextstep])\n",
    "            \n",
    "            done,observation,reward,extra = self.env.step(nextstep)\n",
    "            actions.append(nextstep)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.backprog((states,rewards,actions,prob_pis,prob_bes))\n",
    "        \n",
    "    def estimate_value(self,steps):\n",
    "        for one_step in tqdm.tqdm(range(steps)):\n",
    "            self.monte_carlo_run()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off policy Monte Carol Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceTrackGenerator = RaceTrackGenerator()\n",
    "raceTrackGenerator.generate()\n",
    "\n",
    "raceTrackEnv = RaceTrackEnv(\n",
    "    raceTrackGenerator.plane,\n",
    "    raceTrackGenerator.start_points,\n",
    "    raceTrackGenerator.end_points)\n",
    "\n",
    "behaviorStrategy = EqualProbStrategy(raceTrackEnv)\n",
    "targetStrategy = TargetStrategy(raceTrackEnv)\n",
    "\n",
    "raceCarOptimizer = RaceCarOptimizer(raceTrackEnv,targetStrategy,behaviorStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:55<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "raceCarOptimizer.estimate_value(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可视化下这玩意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_racetrack():\n",
    "    raceTrackEnv.reset()\n",
    "    while True:\n",
    "        state = raceTrackEnv.observation_to_state(raceTrackEnv.get_observation())\n",
    "\n",
    "        nextTargetProbs = targetStrategy.action_probs()\n",
    "\n",
    "        nextactions = list(nextTargetProbs.keys())\n",
    "        nextstep = np.random.choice(list(range(len(nextactions))),p=list(nextTargetProbs.values()))\n",
    "        nextstep = nextactions[nextstep]\n",
    "\n",
    "        done,observation,reward,extra = raceTrackEnv.step(nextstep)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    paintplane = raceTrackGenerator.plane.copy()\n",
    "    for one_state,one_action in raceTrackEnv.history:\n",
    "        paintplane[tuple(one_state[:2])] = 3\n",
    "\n",
    "    return paintplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i + 1)\n",
    "    plt.imshow(generate_one_racetrack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现off policy MC control可以很好的handle这种简单的赛车游戏，但是在更复杂的游戏里，这种简单的Mc control 几乎可以确定无法适用，原因如下：\n",
    "\n",
    "- 在Mc control中我们使用了一个random behavoir policy，这种random behavior policy在复杂的游戏中有可能几乎没法拿到reward（比如在超级玛丽，通关才算reward的时候）\n",
    "\n",
    "- 在off policy MC control back prob的时候，又一个很严苛的条件 if one_action_ind ！= best_action_ind: break，这说明如果behavior policy不能产生最佳招法，其产生的track 就无法被学习到\n",
    "\n",
    "这些条件会使得学习非常困难。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
